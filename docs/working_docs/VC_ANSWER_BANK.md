# VC Application Answer Bank — Canonical

**Purpose**: Canonical written answers for VC/accelerator applications. VC-agnostic — adapt per application as needed.
**Source**: IR Deck v15 + NARRATIVE.md v2 + ESSENCE.md
**Date**: 2026-03-02 (v2 — reframed to honest-bet / platform-cycle thesis)

**How to use**: Copy-paste the canonical version, then trim or expand to fit each application's word limits and tone. Field mappings for specific applications are noted in brackets.

---

## 1. One-Liner

> yarnnn is an autonomous AI agent that connects to your work tools, accumulates your context over time, and produces your deliverables on schedule — getting smarter every cycle.

**Shorter variant** (for tight character limits):
> The application layer for work context — an autonomous AI agent that learns your work and produces deliverables on schedule.

`[PearX: field 6 — "One-line description of the company"]`
`[YC: "Describe what your company does in 50 characters or less"]`

---

## 2. What We're Building & Why

The prevailing assumption in AI right now is that the big LLM providers — OpenAI, Anthropic, Google — will own every layer of the stack. They're building coding agents, desktop tools, browser extensions, memory features. It feels like they'll just do everything.

We think the pattern will rhyme with every prior platform cycle. Google didn't become Salesforce. Facebook didn't become Shopify. AWS didn't become Datadog. General-purpose platforms always look invincible — until the application layer emerges and proves that domain-specific value requires different architecture, different data models, and different product priorities than the platform provider optimizes for. The first application layer the LLM providers built was code — the easiest case, where model capability maps directly to output. Work context is the hard case: unstructured, personal, cross-platform, and domain-specific. Nobody is building that layer yet.

yarnnn is building it. We built TP — an autonomous AI agent that connects to your Slack, Gmail, Notion, and Calendar. It syncs your work context automatically, accumulates knowledge over time, and produces deliverables on schedule — weekly client updates, project status summaries, meeting prep briefs, investor reports — without being asked. You supervise the output. TP operates.

The product is live at yarnnn.com with all four integrations syncing. TP works in two modes: a chat interface where it already has your context, and a headless mode that generates deliverables in the background on a schedule. After 90 days of accumulated context, the system knows your preferences, your clients' priorities, and your communication style. At that point, switching to a competitor means starting from zero again.

`[PearX: field 10 — "What are you building, and why?"]`

---

## 3. Unique Insight

Context is what makes autonomy meaningful — and it's the application layer that cross-platform, interoperability will prevent existing companies be impossible to cross, but makes work more meaningful.

Every AI company is racing toward autonomy — agents that can act without human prompting. But autonomy without context is just random. An autonomous agent that doesn't know your clients, your projects, or your preferences produces generic work that still requires you to review, edit, and redo. That's not autonomy. That's delegation to an intern who never learns.

yarnnn's insight is that the value of an autonomous agent compounds with accumulated context, and try and connect existing work platforms' data as much as possible. Every sync cycle deepens what the system knows. Every edit you make teaches it something. After 90 days, your AI is incomparably better than day one — and a new competitor starts from zero. But the insight goes deeper: work itself is shifting from human-first to agent-first. Today, professionals direct AI. Tomorrow, AI agents will coordinate with other AI agents to execute complex work. In both cases, the critical substrate is the same — persistent, accumulated understanding of the work. That's not a model problem. It's an application-layer problem. And it's the layer LLM providers have never built in any platform cycle.

This means the moat isn't the model, the integrations, or the UI. The moat is the accumulated understanding of each user's work world — and it deepens automatically with every cycle.

`[PearX: field 11 — "What unique insight do you have into this problem?"]`

---

## 4. Traction & Progress

The MVP is live at yarnnn.com with paying tiers (Free / Starter $9/mo / Pro $19/mo). All four platform integrations — Slack, Gmail, Notion, and Google Calendar — are connected and syncing. TP operates in both chat mode (context-aware conversations) and headless mode (scheduled autonomous deliverables).

The technical foundation is unusually deep for a pre-seed company: 80+ Architecture Decision Records document every design choice, from the unified content layer to the retention-based accumulation model. This isn't a wrapper on an API — it's a purpose-built context engine designed from day one for cross-platform context accumulation and agent interoperability (MCP).

The strongest demand signal came from ClawdBot (also called OpenClaw), an open-source project by a separate team that went viral in January 2026: 17,830 GitHub stars in 24 hours — the fastest single-day growth in GitHub history. What drove that explosion wasn't a better chatbot — it was the promise of AI that's yours: personalized, persistent, and capable of operating in your context. But 95% of users couldn't actually use it — it required VPS provisioning, had 200+ security leaks flagged by GitGuardian, and offered no recurring workflows. yarnnn is the productized version of what ClawdBot proved people want.

We're now testing two core hypotheses: (1) that TP already knows your work from Day 1 — eliminating the cold-start problem, and (2) that accumulated context produces measurably better output over time, with edit distance decreasing as tenure increases.

`[PearX: field 12 — "How far along are you?"]`

---

## 5. Competitors & Differentiation

The competitive landscape splits into three categories, and yarnnn is the only product that bridges the gap between them:

**General AI assistants (ChatGPT, Claude, Gemini)**: Extraordinary general-purpose models with improving memory and expanding tools. But their architecture is model-centric, not context-centric — they optimize for breadth across all users, not depth for any specific user. They accumulate some preferences; they don't accumulate your full work context across four platforms and use it for autonomous scheduled output. They're the engine — we're building the vehicle.

**Autonomous agent startups (Devin, AutoGPT, CrewAI)**: These can act independently, but generically. They have no persistent understanding of your specific clients, projects, or preferences. Autonomy without accumulated context produces impressive demos but weak repeat performance for the same user over time.

**Workspace AI (Notion AI, Glean, Granola)**: These have context, but they're trapped inside one platform and produce no autonomous output. Notion AI knows your Notion pages but nothing about your Slack conversations or emails. Glean is enterprise-only with no prosumer play.

yarnnn is the only product that accumulates cross-platform work context and uses it for autonomous output. TP connects to all four platforms, synthesizes across them, and produces deliverables on schedule. The compounding dynamic means switching costs increase with tenure — after 90 days, no competitor can replicate what the system knows about your work. And the structural argument is historical: in every platform cycle, the platform provider didn't build the application layer. We're betting this cycle rhymes.

The comparable companies that have proven this market — Notion ($11B), Glean ($7.2B), Granola ($250M), Mem.ai ($110M), Limitless (acquired by Meta) — all validated demand for AI-powered context. yarnnn adds the autonomous output layer that none of them have.

`[PearX: field 15 — "Who are your competitors? How do you differentiate?"]`

---

## 6. Market Size

We size the market bottoms-up from our entry wedge: solo consultants with recurring client obligations.

There are approximately 5 million solo consultants globally who manage multiple clients, produce recurring deliverables (weekly updates, monthly reports, strategic briefs), and use 3+ productivity tools daily. These are the users who feel the context gap most acutely — they need AI that understands their specific work across platforms, not just a general-purpose assistant.

At our Pro tier pricing ($228/year), the Serviceable Addressable Market is approximately $1.14 billion. Our Entry SOM targets 50,000 consultants (1% of SAM) within the first 3 years, representing $11.4 million in ARR.

The Total Addressable Market for AI productivity tools is $4.35 billion and growing at 31% CAGR. Our expansion path moves from solo consultants to founders and executives (investor updates, board prep, strategic briefs), then to teams and ops leads (standups, cross-team synthesis), and ultimately to all knowledge workers with recurring work obligations. As work shifts from human-first to agent-first, the context layer becomes infrastructure — not just a productivity tool.

The wedge is narrow by design — solo consultants have the clearest pain, the shortest sales cycle, and the highest willingness to pay for tools that eliminate hours of context re-entry per week.

`[PearX: field 16 — "How big is the market opportunity?"]`

---

## 7. Novel Problem-Solving / Origin Story

In 2015, I was tasked with deploying a CRM system for Japan Tobacco International in Myanmar — a country that had only recently begun opening its economy after decades of military rule. The constraints were unlike anything I'd encountered: unreliable electricity, minimal mobile phone penetration, low baseline digital literacy among both internal teams and external distributors, and virtually no enterprise software infrastructure to build on.

There was no playbook. I had to reason from first principles about every layer — from physical infrastructure (how do you run a CRM when the power cuts out three times a day?) to user adaptation (how do you onboard distributors who have never used a database?) to connectivity (how do you sync field data when mobile networks barely exist outside Yangon?). Every assumption that enterprise software takes for granted — reliable internet, device availability, basic technical fluency — had to be rethought and rebuilt from the ground up.

We launched successfully within a year. That experience taught me something I've carried into everything since: the hardest problems aren't technical — they're contextual. The technology was available. What made it work was deeply understanding the environment, the constraints, and the people who would actually use it. That's the same instinct behind yarnnn: the AI models are extraordinary, but the missing layer is context — understanding the specific world the technology needs to operate in.

ClawdBot's viral moment in January 2026 (17,830 GitHub stars in 24 hours) validated the market demand. But the conviction to build yarnnn came from a decade of watching context be the difference between technology that works and technology that doesn't.

`[PearX: field 18 — "Please share an example of a time you tackled a problem in a novel way"]`

---

## 8. Why I'm an Outlier / Founder-Market Fit

I've spent my entire career moving between worlds that don't naturally connect — and building the systems that make them legible to each other.

As a Korean-born founder who has worked across the US, Southeast Asia, and global enterprise environments, I've spent a decade navigating contexts that don't translate on their own. Deploying CRM for Japan Tobacco in post-military Myanmar. Building GTM systems for cross-border sales teams. Designing context architectures for consultants who work across five tools and twelve clients. The through-line is always the same: the technology exists, the people exist, the value exists — but nothing connects them. Someone has to build the bridge, and that someone has to understand both sides deeply enough to make it invisible.

That's not just a professional skill — it's how I see the world. Systems that should talk to each other but don't. Context that exists in one place but is invisible in another. The gap between what technology can do and what any specific person actually needs it to do. yarnnn is the product expression of that instinct: the models are extraordinary, but your work context is scattered across platforms, and no one is building the layer that makes AI understand your specific world.

On the technical side: I shipped the entire MVP solo — full-stack application (Next.js + FastAPI + Supabase), four platform integrations, a unified agent architecture documented across 80+ Architecture Decision Records, and a working context accumulation engine — all before raising a dollar. That's the ability to make hundreds of architectural decisions under uncertainty and ship a coherent product alone.

The combination is rare: a systems thinker shaped by navigating between disconnected worlds, deep domain expertise in the exact problem space, and the technical ability to build the full stack solo. I don't just understand the context problem — I've been living it, across cultures and industries, for a decade.

`[PearX: field 19 — "What makes you an outlier?"]`

---

## Appendix: Quick-Reference Variants

### Elevator Pitch (30 seconds)

> Every platform cycle produces an application layer the platform provider doesn't own. LLMs are no different. We built TP — an autonomous AI agent that connects to your Slack, Gmail, Notion, and Calendar, accumulates your work context, and produces your deliverables on schedule. It's the application layer for work — and no one else is building it. Live at yarnnn.com.

### Twitter-Length (~280 chars)

> yarnnn: the application layer for work context. An autonomous AI agent that connects to your tools, accumulates your context, and produces deliverables on schedule. Gets smarter every cycle. Live now → yarnnn.com

### "Why Now" (standalone paragraph)

> ClawdBot proved explosive demand for personalized, context-aware AI in January 2026 — 17,830 GitHub stars in 24 hours. Every platform cycle's application layer forms within 3–5 years of platform maturity. LLMs are 3 years in. No one is building the work context layer. The architecture is built, the product is live, and we're making the bet that this cycle rhymes — just like every one before it. This funding is about speed: a Tech Lead and a GTM Lead to claim the category while the window is open.
